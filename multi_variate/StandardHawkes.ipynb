{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import utils\n",
    "\n",
    "from math import log,exp,sqrt,pi\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as optimize\n",
    "from scipy.special import erf\n",
    "import types\n",
    "import numpy as np\n",
    "      \n",
    "# MultiVariate Point process event\n",
    "class MVPPEvent:\n",
    "    def __init__(self, dim=0 , time=0, dim_name=None):\n",
    "        self.dim = dim\n",
    "        self.time = time\n",
    "        self.dim_name = dim_name\n",
    "    def __str__(self):\n",
    "        s = str(self.dim)+\"*\"+str(self.time)\n",
    "        if(not self.dim_name is None):\n",
    "            s+= \"*\"+self.dim_name\n",
    "        return s\n",
    "    \n",
    "    def __gt__(self,e2):\n",
    "        return self.time > e2.time\n",
    "    def __eq__(self,e2):\n",
    "        return self.time == e2.time\n",
    "    def __lt__(self,e2):\n",
    "        return self.time < e2.time\n",
    "    def __le__(self,e2):\n",
    "        return self.time<=e2.time\n",
    "    def __cmp__(self,e2):\n",
    "        return self.time <= e2.time\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.dim)+\",\"+str(self.time))\n",
    "    \n",
    "class MVPPSequence:\n",
    "    def __init__(self, lstEvents):\n",
    "        assert all(lstEvents[i].time <= lstEvents[i+1].time for i in xrange(len(lstEvents)-1))\n",
    "        self.lstEvents = lstEvents\n",
    "        self.T = lstEvents[-1].time\n",
    "        self.t_0 = lstEvents[0].time\n",
    "        self.TT = self.detangle()\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        for e in self.lstEvents:\n",
    "            s += str(e) +\", \"\n",
    "        return s.rstrip(\",\")\n",
    "    \n",
    "    def detangle(self):\n",
    "        \n",
    "        TT = defaultdict(list)   \n",
    "        for e in self.lstEvents:\n",
    "            TT[e.dim].append(e.time)\n",
    "            #self.T = max(self.T,e.time)\n",
    "            #self.t_0 = min(self.t_0, e.time)\n",
    "   \n",
    "        return TT\n",
    "    def count_dim(self):\n",
    "        return len(self.TT.keys())\n",
    "    \n",
    "    def append(self,evt):\n",
    "        self.lstEvents.append(evt)\n",
    "        self.TT[evt.dim].append(evt.time)\n",
    "        self.T = max(self.T,evt.time)\n",
    "\n",
    "class MultiVariateHawkesProcessModel:\n",
    "    \n",
    "    def __init__(self,dim,l_0=None,A=None,trig_kernel=None):\n",
    "\n",
    "        assert dim>0\n",
    "        self.dim = dim\n",
    "        self.l_0 = l_0\n",
    "        self.A   = A\n",
    "        self.sigma = np.ones(shape=(dim,dim))\n",
    "        \n",
    "        if(self.l_0 is None):\n",
    "            self.l_0 = np.zeros(self.dim)\n",
    "        if(self.A is None):\n",
    "            self.A = np.zeros(shape=(self.dim,self.dim))\n",
    "        \n",
    "        if(trig_kernel is None):\n",
    "            self.trig_kernel = [[ExponentialKernel(beta=(1.0/72))\n",
    "                                 for x in range(self.dim)] for y in range(self.dim)] \n",
    "        self.num_kernel_params = dim*dim\n",
    "        self.eps = 1e-5\n",
    "        self.fit_kernel_params = False\n",
    "        self.num_params = self.dim + self.dim*self.dim + self.num_kernel_params\n",
    "        \n",
    "        self.sequences = None\n",
    "            \n",
    "    # c is the index of dimension            \n",
    "    def lamda_c(self,t,c,seq):\n",
    "        \n",
    "        #s = self.l_0[c] + self.eps #log safe\n",
    "        sum_d = np.zeros(self.dim) # contrib of each dimension till t\n",
    "        \n",
    "        for e in seq.lstEvents:\n",
    "            if(e.time < t):\n",
    "                sum_d[e.dim] += exp(-(t-e.time)/self.sigma[e.dim][c])\n",
    "            \n",
    "        return self.l_0[c] + self.eps + np.dot(sum_d,self.A[:,c]), sum_d\n",
    "    \n",
    "         \n",
    "    #returns a d dim vector           \n",
    "    def lamda(self,t,seq):\n",
    "        l = np.zeros(self.dim) + self.l_0\n",
    "        for e in seq.lstEvents:\n",
    "            if(e.time<t):\n",
    "                for d in xrange(self.dim):\n",
    "                    l[d] += self.A[e.dim][d] * exp( -(t - e.time)/self.sigma[e.dim][d])\n",
    "            else:\n",
    "                break\n",
    "        return sum(l), l\n",
    "                \n",
    "    \n",
    "    def lamda_ub(self,t,l_t, seq):\n",
    "        '''\n",
    "        v = np.arrange(t,t+l_t,1)\n",
    "        lt = map(lambda x: self.lamda(x,seq)[0], v)\n",
    "        return max(lt)\n",
    "        '''\n",
    "        t1 = time.time()\n",
    "        l = np.zeros(self.dim) + self.l_0\n",
    "        for e in seq.lstEvents:\n",
    "            if(e.time<t):\n",
    "                for d in xrange(self.dim):\n",
    "                    l[d] += self.A[e.dim][d] * exp( -(t - e.time)/self.sigma[e.dim][d])\n",
    "            else:\n",
    "                break\n",
    "        t2 = time.time()\n",
    "        return sum(l)\n",
    "    \n",
    "    #def Lambda(self,t,seq):\n",
    "        \n",
    "    # Should be used to compute log likelihood of any generic kernel\n",
    "    def neg_log_likelihood_general(self):\n",
    "        C = len(self.sequences)\n",
    "        ll = 0\n",
    "        \n",
    "        grad = np.zeros(self.num_params)\n",
    "        p    = self.num_kernel_params\n",
    "        grad_kernel = grad[0:p]\n",
    "        grad_l0 = grad[p:p+self.dim]\n",
    "        grad_A = np.reshape(grad[self.dim: (self.dim+1)*self.dim],(self.dim,self.dim))\n",
    "        \n",
    "        tmp_grad_kernel = [[np.zeros(self.trig_kernel[i][j].get_num_params())\n",
    "                            for j in range(self.dim)] for i in range(self.dim)] \n",
    "        \n",
    "        \n",
    "        for seq in self.sequences:\n",
    "            res = 0\n",
    "            TT  = seq.TT\n",
    "            m_T = seq.T\n",
    "            dims = TT.keys()\n",
    "\n",
    "            for m in dims:\n",
    "                if(len(TT[m])==0): continue\n",
    "\n",
    "                for t in TT[m]:\n",
    "                    lmda_d_t,sum_d = self.lamda_c(t,m,seq)\n",
    "                    res+=log(lmda_d_t)\n",
    "                    grad_l0[m]+= 1.0/lmda_d_t\n",
    "                    grad_A[:,m]+= sum_d/lmda_d_t\n",
    "                    \n",
    "                    for e in seq.lstEvents:\n",
    "                        if(e.time < t):\n",
    "                             tmp_grad_kernel[e.dim][m]+= self.A[e.dim][m]*self.trig_kernel[e.dim][m].kernel_grad(t-e.time)\n",
    "                                \n",
    "                    for e in seq.lstEvents:\n",
    "                        if(e.time<t):\n",
    "                            tmp_grad_kernel[e.dim][m] /=lmda_d_t\n",
    "                    \n",
    "                _sum =0       \n",
    "                for n in dims:\n",
    "                    for k in range(len(TT[n])):\n",
    "                        G = self.trig_kernel[n][m].kernel_integral(m_T - TT[n][k])\n",
    "                        _sum = _sum + (self.A[n][m])*G\n",
    "                        \n",
    "                        G_g = self.trig_kernel[n][m].kernel_integral_grad(m_T - TT[n][k])\n",
    "                        grad_A[n][m] -= G\n",
    "                        tmp_grad_kernel[n][m]-=G_g\n",
    "\n",
    "                res = res - self.l_0[m] * m_T - _sum\n",
    "                grad_l0[m] -= m_T\n",
    "            ll += res\n",
    "        p =0\n",
    "        for i in range(self.dim):\n",
    "            for j in range(self.dim):\n",
    "                r = self.trig_kernel[i][j].get_num_params()\n",
    "                grad_kernel[p:p+r] = tmp_grad_kernel[i][j]\n",
    "                p+=r\n",
    "        return 0-ll, 0-grad\n",
    "    \n",
    "    def grad(self):\n",
    "        return self.neg_log_likelihood_general()[1]\n",
    "    \n",
    "    # Only Applicable for exponential kernel. Faster then the above general method.    \n",
    "    def neg_log_likelihood_exp(self):\n",
    "        C = len(self.sequences)\n",
    "        ll = 0\n",
    "        \n",
    "        grad = np.zeros(self.num_params)\n",
    "        p    = 0\n",
    "        grad_kernel = None\n",
    "        grad_l0 = None\n",
    "        grad_A = None\n",
    "        tmp_grad_kernel = None\n",
    "        if(self.fit_kernel_params):\n",
    "            p = self.num_kernel_params\n",
    "            grad_kernel = grad[0:p]\n",
    "            grad_l0 = grad[p:p+self.dim]\n",
    "            grad_A = np.reshape(grad[self.dim: (self.dim+1)*self.dim],(self.dim,self.dim))\n",
    "\n",
    "            tmp_grad_kernel = [[np.zeros(self.trig_kernel[i][j].get_num_params())\n",
    "                                for j in range(self.dim)] for i in range(self.dim)] \n",
    "        else:\n",
    "            grad_l0 = grad[p:p+self.dim]\n",
    "            grad_A = np.reshape(grad[self.dim: (self.dim+1)*self.dim],(self.dim,self.dim))\n",
    "        \n",
    "        for seq in self.sequences:\n",
    "            res = 0\n",
    "            TT  = seq.TT\n",
    "            m_T = seq.T\n",
    "            dims = TT.keys()\n",
    "            for m in dims :\n",
    "                if(len(TT[m])==0): continue\n",
    "\n",
    "\n",
    "\n",
    "                Rdiag    = np.zeros(len(TT[m]))\n",
    "                RNonDiag = np.zeros(len(TT[m]))\n",
    "\n",
    "                index    = map(int,np.zeros(self.dim))\n",
    "\n",
    "                Rdiag[0] = 0\n",
    "\n",
    "                \n",
    "                for i in range(1,len(TT[m])):\n",
    "\n",
    "                    Rdiag[i] = (1.0+Rdiag[i-1])*self.trig_kernel[m][m].kernel(TT[m][i] -TT[m][i-1])\n",
    "\n",
    "                RNonDiag[0] = 0.0\n",
    "\n",
    "                for n in dims:\n",
    "                    if(n==m): continue\n",
    "                    for k in range(len(TT[n])):\n",
    "                        if(TT[n][k]<TT[m][0]):\n",
    "                            \n",
    "                            RNonDiag[0] += self.trig_kernel[n][m].kernel(TT[m][0]-TT[n][k])\n",
    "\n",
    "                for i in range(1,len(TT[m])):\n",
    "\n",
    "                    RNonDiag[i] = (RNonDiag[i-1])*self.trig_kernel[m][m].kernel(TT[m][i] - TT[m][i-1])\n",
    "\n",
    "                    for n in dims:\n",
    "                        if (m==n):\n",
    "                            continue\n",
    "                        for k in range(index[n],len(TT[n])):\n",
    "                            if (TT[n][k] >= TT[m][i-1]):\n",
    "                                if (TT[n][k] < TT[m][i]):\n",
    "\n",
    "                                    RNonDiag[i] += self.trig_kernel[n][m].kernel(TT[m][i] - TT[n][k])\n",
    "                                else:\n",
    "                                    index[n] = k\n",
    "                                    break\n",
    "\n",
    "                for i in range(len(TT[m])):\n",
    "                    s = self.l_0[m] +self.eps\n",
    "                    for n in dims:\n",
    "                        if(m==n):\n",
    "                            s = s + self.A[n][m]*Rdiag[i]\n",
    "                        else:\n",
    "                            s = s + self.A[n][m]*RNonDiag[i]\n",
    "                    \n",
    "                    grad_l0[m] += 1/s\n",
    "                    for n in dims:\n",
    "                        if(m==n):\n",
    "                            grad_A[n][m] += RDiag[i]/s\n",
    "                        else:\n",
    "                            grad_A[n][m] += RNonDiag[i]/s\n",
    "                        \n",
    "                    res = res + log(s)\n",
    "                _sum     = 0.0\n",
    "                for n in dims:\n",
    "                    ls = 0.0\n",
    "                    for k in range(len(TT[n])):\n",
    "                        ls = ls + self.trig_kernel[n][m].kernel_integral(m_T-TT[n][k])\n",
    "                    grad_A[n][m] -= ls\n",
    "                    _sum += ls\n",
    "                    \n",
    "\n",
    "                res = res - self.l_0[m] * m_T -  (self.A[n][m])*_sum                           \n",
    "                grad_l0[m]-=seq.T\n",
    "                \n",
    "            ll += res\n",
    "        return 0-ll\n",
    "    \n",
    "    \n",
    "    def opt_callback_set_params(self,x):\n",
    "        p = 0\n",
    "        if(self.fit_kernel_params):\n",
    "            for i in range(self.dim):\n",
    "                for j in range(self.dim):\n",
    "                    c = self.trig_kernel[i][j].get_num_params()\n",
    "                    self.trig_kernel[i][j].set_params_callback(x[p:p+c])\n",
    "                    p+=c\n",
    "\n",
    "        self.l_0  = x[p:p+self.dim]\n",
    "        p        += self.dim\n",
    "\n",
    "        self.A = np.reshape(x[p:],(self.dim,self.dim))\n",
    "    \n",
    "    def opt_callback_get_init_x(self):\n",
    "        n = 0\n",
    "        x0 = []\n",
    "        if(self.fit_kernel_params):\n",
    "            for i in range(self.dim):\n",
    "                for j in range(self.dim):\n",
    "                    x0.extend(self.trig_kernel[i][j].get_init_params())\n",
    "\n",
    "        n = self.dim + self.dim*self.dim        \n",
    "        x0.extend((1e-8)*np.ones(n))\n",
    "\n",
    "        return np.array(x0)\n",
    "    \n",
    "    def opt_callback_get_bounds(self):\n",
    "        bnds = []\n",
    "        if(self.fit_kernel_params):\n",
    "            for i in range(self.dim):\n",
    "                for j in range(self.dim):\n",
    "                    bnds.extend(self.trig_kernel[i][j].get_bounds())\n",
    "        n = self.dim + self.dim*self.dim        \n",
    "        bnds.extend([(1e-8,10.0)]*(n))\n",
    "        return bnds\n",
    "        \n",
    "    def print_params(self):\n",
    "        if(self.fit_kernel_params):\n",
    "            for i in range(self.dim):\n",
    "                for j in range(self.dim):\n",
    "                    self.trig_kernel[i][j].print_params()\n",
    "        print \"l_0: \" +str(self.l_0)\n",
    "        print \"A:\" + str(self.A)\n",
    "        \n",
    "    def fit_mle_params(self,sequences,max_iters=1000,fit_kernel_params=False,tol=1e-18):\n",
    "        self.sequences = sequences\n",
    "        C = len(self.sequences)\n",
    "        self.fit_kernel_params = fit_kernel_params\n",
    "        self.f = 0\n",
    "        def obj(x):\n",
    "            self.opt_callback_set_params(x)\n",
    "            ll,g = self.neg_log_likelihood_general()\n",
    "            #print ll\n",
    "            self.f+=1 \n",
    "            print ll/C, self.f\n",
    "            return ll/C\n",
    "        \n",
    "        def jac(x):\n",
    "            self.opt_callback_set_params(x)\n",
    "            g = self.grad()\n",
    "            #print x.shape, g.shape\n",
    "            return g/C\n",
    "        \n",
    "        bnds = self.opt_callback_get_bounds()\n",
    "        x0   = self.opt_callback_get_init_x()\n",
    "        \n",
    "        res = optimize.minimize(obj, x0= x0,jac=False,method=\"L-BFGS-B\",bounds=bnds,\n",
    "                                tol=tol,options={\"disp\":True,\"maxiter\":max_iters,\"maxfun\":max_iters})\n",
    "        \n",
    "        self.opt_callback_set_params(res.x)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    def fit_mle_params(self,prefix=None,df_pd_train=None,dc=None,tol=1e-18,fit_kernel_params=True):\n",
    "        mle_obj = mle.MaximumLikelihoodEstimator(model = self,prefix=prefix, df_pd_train=df_pd_train,\n",
    "        dc=dc,tol=tol)\n",
    "        self.fit_kernel_params = fit_kernel_params\n",
    "        mle_obj.estimate()\n",
    "    '''     \n",
    "    def predictNextEventTime(self,seq,num_sim,step=1):\n",
    "        ot = OgataThinning()\n",
    "        t = 0\n",
    "        for i in range(num_sim):\n",
    "            event = ot.SimulateNext(self,1000,seq,step)\n",
    "            t += event.time\n",
    "        return float(t) / num_sim\n",
    "    def predictNextKEvents(self,seq,k=5,num_sim=25,step=1):\n",
    "        n = len(seq.lstEvents)\n",
    "        import copy\n",
    "        seq = MVPPSequence(copy.deepcopy(seq.lstEvents))\n",
    "        \n",
    "        for j in range(k):\n",
    "            t = self.predictNextEventTime(seq,num_sim,step)\n",
    "            s,l= self.lamda(t,seq)\n",
    "            i = np.argmax(l)\n",
    "            e = MVPPEvent(time=t,dim=i)\n",
    "            print j,e\n",
    "            seq.append(e)\n",
    "        return seq.lstEvents[n:]\n",
    "    \n",
    "    #def predictItem(self,seq, t, num_sim):\n",
    "        \n",
    "    \n",
    "    def plot(self,seq):\n",
    "        \n",
    "        def __plot(T,d,t_min,t_max,label):\n",
    "            print label\n",
    "            fig = plt.figure(figsize=(20,5))\n",
    "            ax1 = fig.add_subplot(311)\n",
    "\n",
    "            ax1.set_xlim([t_min, t_max])\n",
    "\n",
    "            v = np.arange(t_min,t_max, 0.1)\n",
    "            v = v[v<=t_max]\n",
    "            lamdas = map(lambda t: self.lamda_c(t,d,seq)[0], v)\n",
    "\n",
    "            a1, = ax1.plot(v,lamdas,label=label)\n",
    "\n",
    "            ax3 = fig.add_subplot(312)\n",
    "            ax3.set_xlim([t_min, t_max])\n",
    "            for t_i in T:\n",
    "                ax3.axvline(t_i)\n",
    "            plt.legend(handles=[a1])\n",
    "            plt.show()\n",
    "            \n",
    "        TT = seq.TT\n",
    "        dim_id_name = dict()\n",
    "        for e in seq.lstEvents:\n",
    "            dim_id_name[e.dim] = e.dim_name\n",
    "        t_0 = seq.t_0-100\n",
    "        T   = seq.T+100\n",
    "                         \n",
    "        for d in range(self.dim):\n",
    "            if(len(TT[d])>0):\n",
    "                label = dim_id_name[d]\n",
    "                __plot(TT[d],d,t_0,T,label)\n",
    "    '''\n",
    "    def check_stability(self):\n",
    "        if(self.trig_kernel== self.exp)            \n",
    "    '''       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
