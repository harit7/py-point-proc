{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import exp,log\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from os import sys\n",
    "import numpy as np\n",
    "from scipy.special import lambertw\n",
    "from scipy.optimize import  bisect,newton\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "sys.path.append(\"./lib_point_proc\")\n",
    "\n",
    "from basic_models import IntensityBasedPointProcessModel\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "\n",
    "class HTRm (IntensityBasedPointProcessModel) :\n",
    "    \n",
    "    def __init__(self,r_0=0.0,a_s=0.0,b_s=0.0,T_s=[],a_h=0.0,b_h=0.0,kappa=0.0,\n",
    "                 all_realizations=defaultdict(list),cur_realization_key=\"\"):\n",
    "\n",
    "        self.r_0 = r_0\n",
    "        self.a_s = a_s\n",
    "        self.b_s = b_s\n",
    "        self.a_h = a_h\n",
    "        self.b_h = b_h\n",
    "        self.kappa = kappa\n",
    "        self.T_s = T_s\n",
    "        \n",
    "        self.all_realizations = all_realizations\n",
    "        self.cur_realization_key = cur_realization_key\n",
    "        \n",
    "        self.__d_lam_j  =0.0\n",
    "        self.__lam_j_1  =0.0\n",
    "        \n",
    "    def lamda_sale(self,t):\n",
    "        \n",
    "        T   = self.get_cur_realization()\n",
    "        T_s = self.T_s\n",
    "        \n",
    "        s   = 0\n",
    "        for j in range(0,len(T_s)):\n",
    "            if(T_s[j]<=t):\n",
    "                s += (t-T_s[j])*exp(- self.b_s*(t-T_s[j]))\n",
    "\n",
    "        return self.r_0 * (1 + self.a_s * s)\n",
    "\n",
    "    def Lamda_sale(self,t):\n",
    "        T   = self.get_cur_realization()\n",
    "        T_s = self.T_s\n",
    "        a_s = self.a_s\n",
    "        b_s = self.b_s\n",
    "        r_0 = self.r_0\n",
    "        t_0 = T[0]\n",
    "\n",
    "        s = 0\n",
    "        d = 1.0/b_s\n",
    "\n",
    "        for i in range(0,len(T_s)):\n",
    "            if(T_s[i]<=t):\n",
    "                h = max(0, t_0 - T_s[i])\n",
    "                g = max(0, t   - T_s[i])\n",
    "                s = s + ( exp(-b_s*h)*(h + d) ) - ( exp(-b_s*g)*( t - T_s[i] + d)  )\n",
    "        s = r_0*(t-t_0) + (r_0*a_s*d) * s\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "    def lamda_excite(self,t,L_t, L_T):\n",
    "\n",
    "        a_h = self.a_h\n",
    "        b_h = self.b_h\n",
    "        T   = self.get_cur_realization()\n",
    "\n",
    "        s   = 0\n",
    "        for j in range(0,len(T)):\n",
    "            if(T[j]<t):\n",
    "                try:\n",
    "                    s += exp(-b_h*(L_t-L_T[j]))\n",
    "                except:\n",
    "                    print b_h, L_t, L_T[j]\n",
    "\n",
    "        return (1 + a_h * s)\n",
    "        \n",
    "\n",
    "    def Omega(self,t,L_t, L_T):\n",
    "        \n",
    "        a_h = self.a_h\n",
    "        b_h = self.b_h\n",
    "        T   = self.get_cur_realization()\n",
    "        \n",
    "        s = 0\n",
    "        for j in range(0,len(T)):\n",
    "            if(T[j]<t):\n",
    "                try:\n",
    "                    s =  s + 1 - exp(-b_h*(L_t- L_T[j]))\n",
    "                except:\n",
    "                    print \"error:\" + str((L_t, L_T[j], b_h, T, L_T ))\n",
    "\n",
    "        return L_t+ (float(a_h)/b_h)*s\n",
    "\n",
    "    \n",
    "    def lamda_pre(self,t,O_t,O_T):\n",
    "        \n",
    "        T     =  self.get_cur_realization()\n",
    "        kappa =  self.kappa\n",
    "        n_t   = self.N_t(t)\n",
    "\n",
    "        if(len(T) ==0 or n_t ==0):\n",
    "\n",
    "            return 1.0\n",
    "        if(n_t == 1 and t == T[0]):\n",
    "            print \"n_t 1\"\n",
    "            return kappa*(1.0)\n",
    "        d = O_t-O_T[n_t-1]\n",
    "        if(d<0):\n",
    "            print \"delta \"+ str(d)\n",
    "        return kappa * utils.safe_pow(O_t-O_T[n_t-1],kappa-1) \n",
    "\n",
    "\n",
    "    def Psi(self,t,O_t,O_T):\n",
    "        \n",
    "        T     =  self.get_cur_realization()\n",
    "        kappa =  self.kappa\n",
    "\n",
    "        n_t  = self.N_t(t)\n",
    "        if(len(T)==0 or n_t ==0):\n",
    "            return 0.0\n",
    "\n",
    "        s = 0    \n",
    "        for j in range(1,n_t-1):\n",
    "            #print j,n_t,len(T),len(O_T)\n",
    "            s += utils.safe_pow(O_T[j+1] - O_T[j],kappa)\n",
    "\n",
    "        ret = s + utils.safe_pow(O_t - O_T[n_t-1],kappa)\n",
    "\n",
    "        return ret\n",
    "    \n",
    "\n",
    "    def lamda(self,t):\n",
    "\n",
    "\n",
    "        T = self.get_cur_realization()\n",
    "        \n",
    "        L_T = map(self.Lamda_sale,T)\n",
    "        O_T = []\n",
    "        for i in range(len(T)):\n",
    "            O_T.append(self.Omega(T[i],L_T[i], L_T))\n",
    "\n",
    "        L_t = self.Lamda_sale(t)\n",
    "        O_t = self.Omega(t, L_t, L_T)\n",
    "        \n",
    "        return self.lamda_sale(t)*self.lamda_excite(t,L_t,L_T)*self.lamda_pre(t,O_t,O_T)\n",
    "    \n",
    "\n",
    "    def Lamda(self):\n",
    "        raise(\"Not Implemented\")\n",
    "        \n",
    "    def log_lamda_all(self,t,L_T,O_T):\n",
    "        \n",
    "        T   =  self.get_cur_realization()\n",
    "        \n",
    "        L_t = self.Lamda_sale(t)\n",
    "\n",
    "        O_t = self.Omega(t,L_t, L_T)\n",
    "\n",
    "        l_sal_t = self.lamda_sale(t)\n",
    "        l_exc_t = self.lamda_excite(t,L_t,L_T)\n",
    "        l_pre_t = self.lamda_pre(t,O_t,O_T)\n",
    "\n",
    "\n",
    "        if(l_sal_t <0 or l_exc_t<0 or l_pre_t <0):\n",
    "            print \"-ve\" + str((l_sal_t,l_exc_t, l_pre_t))\n",
    "\n",
    "        if(l_sal_t == 0 or l_exc_t==0 or l_pre_t == 0):\n",
    "            print \"zero\" + str((l_sal_t,l_exc_t, l_pre_t))\n",
    "\n",
    "        if(l_sal_t ==0):\n",
    "            print \"l_sal_t=0\"\n",
    "            l_sal_t = 1.0 \n",
    "        if(l_exc_t ==0):\n",
    "            print \"l_exc_t=0\"\n",
    "            l_exc_t = 1.0\n",
    "        if(l_pre_t ==0):\n",
    "            print \"l_pre_t=0\"\n",
    "            l_pre_t = 1.0\n",
    "\n",
    "\n",
    "        return log(l_sal_t) + log(l_exc_t ) + log(l_pre_t)\n",
    "    \n",
    "    def log_likelihood_1(self):\n",
    "        \n",
    "        T     =  self.get_cur_realization()\n",
    "\n",
    "        t_max = T[-1]\n",
    "        L_T = map(self.Lamda_sale,T)\n",
    "        \n",
    "        O_T = []\n",
    "        for i in range(len(T)):\n",
    "            O_T.append(self.Omega(T[i],L_T[i],L_T))\n",
    "\n",
    "        L_t_mx = self.Lamda_sale(t_max)\n",
    "        O_t_mx = self.Omega(t_max, L_t_mx, L_T) \n",
    "\n",
    "        ll = 0\n",
    "        for t in T:\n",
    "            l = self.log_lamda_all(t, L_T, O_T)\n",
    "            ll += l\n",
    "\n",
    "        return ll - self.Psi(t_max, O_t_mx, O_T)\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        \n",
    "        T     =  self.get_cur_realization()\n",
    "        t_max = T[-1]\n",
    "        \n",
    "        a_h   = self.a_h\n",
    "        b_h   = self.b_h\n",
    "\n",
    "        kappa = self.kappa\n",
    "\n",
    "        L_T     = map(self.Lamda_sale, T)\n",
    "        L_t_max = L_T[-1]\n",
    "\n",
    "        sale_s = 0\n",
    "        for t in T:\n",
    "            sale_s += log(self.lamda_sale(t))\n",
    "\n",
    "        n = len(T)\n",
    "        A = np.zeros(n)\n",
    "        hawk_s = 0\n",
    "        for j in range(0,n-1):\n",
    "\n",
    "            A[j+1] = (1+A[j])*exp(-b_h*(L_T[j+1] - L_T[j]))\n",
    "            hawk_s += log(1+ a_h*A[j])\n",
    "\n",
    "        hawk_s += log(1+a_h*A[n-1])\n",
    "\n",
    "        hawk_s_1= 0\n",
    "        for j in range(0,n):\n",
    "            hawk_s_1 += log(self.lamda_excite(T[j],L_T[j],L_T))\n",
    "\n",
    "        pre_s = 0\n",
    "        D_omg = np.zeros(n)\n",
    "        s1 = 0\n",
    "        s2 = 0\n",
    "\n",
    "\n",
    "        for j in range(1,n-1):\n",
    "            D_omg[j] = L_T[j+1] - L_T[j] + (a_h/b_h)*(1 + A[j] - A[j+1])\n",
    "            '''\n",
    "            if(L_T[j+1]-L_T[j]<0):\n",
    "                print L_T[j+1],L_T[j]\n",
    "            if(1 + A[j] - A[j+1]<0):\n",
    "                print  A[j],A[j+1]\n",
    "            '''    \n",
    "            if(D_omg[j]<0):\n",
    "                print\"D_omg\"+ str((D_omg[j],L_T[j+1],L_T[j],A[j],A[j+1]))\n",
    "                D_omg[j]=1.0\n",
    "            if(D_omg[j]==0):\n",
    "                D_omg[j]=1.0\n",
    "            s1 += log(D_omg[j])\n",
    "            s2 += (D_omg[j])**(kappa)\n",
    "\n",
    "        pre_s = (n-1)*log(kappa) + (kappa-1)* s1 - s2\n",
    "        # O_T[1] = L_T[1]  eqn 18\n",
    "\n",
    "        ex = L_t_max - L_T[-1] + (a_h/b_h)*(1+ A[-1])*(1- exp(-b_h*(L_t_max- L_T[-1]))) \n",
    "\n",
    "        #print sale_s, hawk_s, pre_s, L_T[1]**kappa, ex**kappa\n",
    "        #print hawk_s_1\n",
    "        return sale_s + hawk_s + pre_s - L_T[1]**kappa - ex**kappa\n",
    "    \n",
    "    def fit_ml e_params(self,tol=1e-15):\n",
    "        \n",
    "        def obj(x):\n",
    "            if(min(x)<0):\n",
    "                return 10000000\n",
    "            self.r_0  = x[0]\n",
    "            self.a_s  = x[1]\n",
    "            self.b_s  = x[2]\n",
    "            self.a_h  = x[3]\n",
    "            self.b_h  = x[4]\n",
    "            self.kappa = x[5]\n",
    "            \n",
    "            l = -self.log_likelihood()\n",
    "            return l\n",
    "                \n",
    "\n",
    "        bnds = ((0.00001, 10.0), (0.00001,10.0), (0.00001,10.0),(0.00001,10.0),(0.00001,10.0),(0.00001,10.0))\n",
    "        res = optimize.minimize(obj, x0= (0.00001,0.00001,0.00001,0.00001,0.00001,0.00001),\n",
    "                                method=\"L-BFGS-B\",bounds=bnds,tol=tol)\n",
    "         \n",
    "        self.r_0  = res.x[0]\n",
    "        self.a_s  = res.x[1]\n",
    "        self.b_s  = res.x[2]\n",
    "        self.a_h  = res.x[3]\n",
    "        self.b_h  = res.x[4]\n",
    "        self.kappa = res.x[5]\n",
    "        \n",
    "        return res\n",
    "    \n",
    "\n",
    "    \n",
    "    #def fit_map_params(self):\n",
    "    def fit_em_map_params(self,max_iters=50,threshold=0.01):\n",
    "        from scipy.special import psi\n",
    "        from scipy.optimize import  bisect,newton\n",
    "        from pymc import Gamma,MCMC\n",
    "        import pymc\n",
    "                \n",
    "        def __converged(c_mu, p_mu,tol=0.01):\n",
    "\n",
    "            for k in c_mu.keys():\n",
    "                if(abs(c_mu[k]-p_mu[k])>tol):\n",
    "                    return False\n",
    "\n",
    "            return True\n",
    "\n",
    "\n",
    "        def __nu_update_func(x):\n",
    "\n",
    "            if(x<=0.0):\n",
    "                x = 0.00001\n",
    "            return log(x) - psi(x) - log(mu_c[cur_k]) + e_log_theta_dict[cur_k]\n",
    "\n",
    "        T = self.get_cur_realization()\n",
    "\n",
    "        mu_p = {\"a_s\":1.0,  \"b_s\":1.0,  \"r_0\":1.0,\"a_h\":1.0, \"b_h\":2.0,  \"kappa\":1.0}\n",
    "        nu_p = {\"a_s\":1.0,  \"b_s\":1.0,  \"r_0\":1.0,\"a_h\":1.0,  \"b_h\":1.0,  \"kappa\":1.0}\n",
    "\n",
    "        mu_c = mu_p.copy()\n",
    "        nu_c = mu_p.copy()\n",
    "        \n",
    "        cur_k = \"a_s\"\n",
    "        e_log_theta_dict = {}\n",
    "\n",
    "        param_names = [\"a_s\",\"b_s\",\"r_0\",\"a_h\",\"b_h\",\"kappa\"]\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        ll_ar = []\n",
    "\n",
    "        while i < max_iters :\n",
    "\n",
    "            a_s = Gamma( 'a_s', alpha=nu_c[\"a_s\"], beta=nu_c[\"a_s\"]/mu_c[\"a_s\"])\n",
    "            b_s = Gamma( 'b_s', alpha=nu_c[\"b_s\"], beta=nu_c[\"b_s\"]/mu_c[\"b_s\"])\n",
    "            r_0 = Gamma( 'r_0', alpha=nu_c[\"r_0\"], beta=nu_c[\"r_0\"]/mu_c[\"r_0\"])\n",
    "\n",
    "            a_h = Gamma( 'a_h', alpha=nu_c[\"a_h\"], beta=nu_c[\"a_h\"]/mu_c[\"a_h\"])\n",
    "            b_h = Gamma( 'b_h', alpha=nu_c[\"b_h\"], beta=nu_c[\"b_h\"]/mu_c[\"b_h\"])\n",
    "\n",
    "            kappa = Gamma( 'kappa', alpha=nu_c[\"kappa\"], beta=nu_c[\"kappa\"]/mu_c[\"kappa\"])\n",
    "\n",
    "\n",
    "\n",
    "            c = (mu_c[\"a_h\"]+mu_c[\"b_h\"])\n",
    "            p_h = {\"a_s\":mu_c[\"a_s\"], \"b_s\":mu_c[\"b_s\"], \"r_0\":mu_c[\"r_0\"], \"kappa\":mu_c[\"kappa\"], \n",
    "                   \"a_h\":mu_c[\"a_h\"]/c, \"b_h\":mu_c[\"b_h\"]/c, \"T_s\":T_s}\n",
    "\n",
    "            t_max = T[-1]\n",
    "\n",
    "            @pymc.stochastic(observed=True)\n",
    "            def htrm_stochastic(value=T,a_s =a_s, b_s=b_s,r_0=r_0, \n",
    "                                  a_h=a_h, b_h = b_h, kappa =kappa ):\n",
    "\n",
    "                #params = {\"a_s\":a_s, \"b_s\":b_s, \"r_0\":r_0, \"a_h\":a_h,\"b_h\":b_h, \"kappa\":kappa, \"T_s\":T_s,\"t_0\":t_0}\n",
    "                self.a_s = a_s\n",
    "                self.b_s = b_s\n",
    "                self.r_0 = r_0\n",
    "                self.a_h = a_h\n",
    "                self.b_h = b_h\n",
    "                self.kappa = kappa\n",
    "                \n",
    "                l      = self.log_likelihood_1()\n",
    "                return l\n",
    "\n",
    "\n",
    "            model = MCMC([htrm_stochastic,a_s,b_s,r_0,a_h,b_h,kappa])\n",
    "            #model.use_step_method(pymc.Metropolis, a_s, proposal_sd=1., proposal_distribution='Normal')\n",
    "\n",
    "            model.sample(iter=1000, burn=100,thin=80)\n",
    "            mu_p = mu_c.copy()\n",
    "            nu_p = nu_c.copy()\n",
    "\n",
    "            for p in param_names:\n",
    "                samples = model.trace(p)[:]\n",
    "                mu_c[p] = samples.mean()\n",
    "                e_log_theta_dict[p] = np.array(map(log,samples)).mean()\n",
    "\n",
    "            for k in mu_c.keys():\n",
    "                cur_k = k\n",
    "                #nu[cur_mu] = bisect(func2,0.01, 100000)\n",
    "                nu_c[cur_k] = newton(__nu_update_func,nu_p[cur_k],tol=0.0001,maxiter=10000)\n",
    "\n",
    "            c = (mu_c[\"a_h\"]+mu_c[\"b_h\"])\n",
    "            print \"\\n iteration: \"+ str(i)\n",
    "\n",
    "            print mu_c[\"a_s\"],mu_c[\"b_s\"],mu_c[\"r_0\"],mu_c[\"a_h\"],mu_c[\"b_h\"],mu_c[\"kappa\"],mu_c[\"a_h\"]/c, mu_c[\"b_h\"]/c\n",
    "            mu_c[\"a_h\"] /=c\n",
    "            mu_c[\"b_h\"] /=c\n",
    "\n",
    "            p_h = {\"a_s\":mu_c[\"a_s\"], \"b_s\":mu_c[\"b_s\"], \"r_0\":mu_c[\"r_0\"], \"kappa\":mu_c[\"kappa\"], \n",
    "                   \"a_h\":mu_c[\"a_h\"]/c, \"b_h\":mu_c[\"b_h\"]/c, \"T_s\":T_s}\n",
    "\n",
    "            self.a_s = mu_c[\"a_s\"]\n",
    "            self.b_s = mu_c[\"b_s\"]\n",
    "            self.r_0 = mu_c[\"r_0\"]\n",
    "            self.a_h = mu_c[\"a_h\"]\n",
    "            self.b_h = mu_c[\"b_h\"]\n",
    "            self.kappa = mu_c[\"kappa\"]\n",
    "            \n",
    "            if(__converged(mu_c,mu_p,0.01)):\n",
    "                print \"Converged\"\n",
    "                break\n",
    "            i+=1\n",
    "\n",
    "\n",
    "    def sample(self,t_0=0,t_max=1000,n_samples=100):\n",
    "        \n",
    "        def __func(t):\n",
    "\n",
    "            lam = self.Lamda_sale(t)\n",
    "\n",
    "            return lam - lam_j_1 - d_lam_j   \n",
    "\n",
    "\n",
    "        def __gradient(t):\n",
    "            r_0  = self.r_0\n",
    "            a_s  = self.a_s\n",
    "            b_s  = self.b_s\n",
    "\n",
    "            s   = 0\n",
    "            for j in range(0,len(T_s)):\n",
    "                if(T_s[j]<=t):\n",
    "                    s += (t-T_s[j])*exp(-b_s*(t-T_s[j]))\n",
    "\n",
    "            return r_0 * (1 + a_s * s)\n",
    "        \n",
    "        T       = np.zeros(n_samples)\n",
    "        self.all_realizations[\"cur_sample\"]= T\n",
    "        self.cur_realization_key = \"cur_sample\"\n",
    "        \n",
    "\n",
    "        T_s  = self.T_s\n",
    "\n",
    "        d_lam_j = 0.0\n",
    "\n",
    "        psi     = [0]\n",
    "        omg     = [0]\n",
    "\n",
    "        b_h     = self.b_h\n",
    "        a_h     = self.a_h\n",
    "        kappa   = self.kappa\n",
    "        \n",
    "        T[0]    = t_0  #???\n",
    "\n",
    "        d_psi = np.random.exponential(1.0, n_samples+1)\n",
    "\n",
    "        d_psi[0]= 0\n",
    "\n",
    "        d_omg = d_psi**(1.0/kappa)\n",
    "\n",
    "        d_lam = np.zeros(len(d_omg))\n",
    "\n",
    "        d_lam[0] = 0.0\n",
    "\n",
    "        B = a_h/b_h\n",
    "\n",
    "        for j in range(1,n_samples):\n",
    "\n",
    "            lam_j_1  = self.Lamda_sale(T[j-1])\n",
    "\n",
    "            d_lam[j]     = d_omg[j] - B + (1.0/b_h) *lambertw( b_h * B * exp(-b_h * ( d_omg[j]-B) ) ).real \n",
    "            B = (a_h/b_h) * ( 1 + B * exp(-b_h * d_lam[j]) )\n",
    "            d_lam_j = d_lam[j]\n",
    "\n",
    "            #t = bisect(self.__func,T[j-1],T[j-1]+1000)\n",
    "\n",
    "            t = newton(__func,T[j-1], tol=0.0001,maxiter=100000)\n",
    "            T[j] = t\n",
    "            #T.sort() # to find the t_j in the neighbourhood of t_{j-1} \n",
    "        return T\n",
    "    \n",
    "    def next_k_sample(self,k=1):\n",
    "\n",
    "        __d_lam_j = 0.0\n",
    "        T_s = self.T_s\n",
    "\n",
    "        psi     = [0]\n",
    "        omg     = [0]\n",
    "\n",
    "        b_h     = self.b_h\n",
    "        a_h     = self.a_h\n",
    "        kappa   = self.kappa\n",
    "\n",
    "        cur_samples = self.get_cur_realization()\n",
    "        \n",
    "        n       = len(cur_samples)\n",
    "        T       = np.zeros(n+k)\n",
    "        np.copyto(T[:n],cur_samples)\n",
    "\n",
    "        d_psi   = np.random.exponential(1.0, len(T)+1)\n",
    "        d_psi[0]= 0\n",
    "\n",
    "        d_omg   = d_psi**(1.0/kappa)\n",
    "\n",
    "        d_lam   = np.zeros(len(d_omg))\n",
    "\n",
    "        d_lam[0]= 0.0\n",
    "\n",
    "        B       = a_h/b_h\n",
    "\n",
    "        for j in range(1,n+k):\n",
    "\n",
    "            self.__lam_j_1  = Lambda_sale(T[j-1],T[:j],params)\n",
    "\n",
    "            d_lam[j] = d_omg[j] - B + (1.0/b_h) *lambertw( b_h * B * exp(-b_h * ( d_omg[j]-B) ) ).real \n",
    "            B        = (a_h/b_h) * ( 1 + B * exp(-b_h * d_lam[j]) )\n",
    "            self.__d_lam_j  = d_lam[j]\n",
    "\n",
    "            if( j>= len(cur_samples) ):\n",
    "\n",
    "                t    = newton(__func,T[j-1]+1, tol=0.001,maxiter=100000)\n",
    "                #t =  bisect(func,T[j-1],T[j-1]+100)\n",
    "\n",
    "                T[j] = t\n",
    "\n",
    "        return T[n:]\n",
    "\n",
    "    def __str__(self):\n",
    "        return str({\"a_s\":self.a_s,\"b_s\":self.b_s,\"r_0\":self.r_0,\"a_h\":self.a_h,\"b_h\":self.b_h,\"kappa\":self.kappa})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1.            9.47694647   15.04343762   19.45771804   20.86731612\n",
      "   22.16116202   22.81904677   28.43696755   43.74959183   44.50293434\n",
      "   48.98289614   52.40886684   56.83017476   76.68209759   77.05522801\n",
      "   77.43265239   77.50919579   87.48705263   97.65877102  112.94666435\n",
      "  146.27926675  157.82541302  161.35641962  171.11989111  171.74170524\n",
      "  177.09130862  177.80479288  199.13912432  203.55358291  206.33204111\n",
      "  207.74605403  216.42072822  219.75473617  224.30291036  224.38724062\n",
      "  229.07481348  234.28314569  239.77230679  262.58655928  270.12138185\n",
      "  272.48387349  275.3170133   282.13513218  282.4249491   288.12492161\n",
      "  294.67275947  295.82877341  296.40823102  297.97553333  299.91671291]\n",
      "-138.464379156\n",
      "1.10463335336\n",
      "{'spearmanr': {'p-value': 0.51170064725614706, 'coef': 0.09704732957012592}, 'pearsonr': {'p-value': 0.23746137418278329, 'coef': 0.17379385692351207}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "T_s = [20,40] # Sales points\n",
    "\n",
    "#params = {\"a_s\":5.0, \"b_s\":0.5, \"r_0\":0.1, \"t_0\":1.0, \"kappa\":1.0, \"a_h\":1.0, \"b_h\":2.0,\"T_s\":T_s}\n",
    "\n",
    "htrm = HTRm(a_s=5.0,b_s=0.5,r_0=0.1,kappa=1.0,a_h=1.0,b_h=2.0,T_s=[20,40])\n",
    "T = htrm.sample(n_samples=50,t_0=1.0)\n",
    "print T\n",
    "#htrm.plot()\n",
    "print htrm.log_likelihood_1()\n",
    "print htrm.coef_variation()\n",
    "print htrm.iei_corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIgAAADVCAYAAAA8c8v5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca2f209790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "htrm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.9 sec\n",
      " iteration: 0\n",
      "1.27803948491 0.84235461762 0.141851421357 0.401544538088 2.85268935394 0.938329578573 0.123391419121 0.876608580879\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.2 sec\n",
      " iteration: 1\n",
      "2.46080532676 0.851136031541 0.142622468398 0.105035928628 1.39996963201 0.955470862799 0.0697910568406 0.930208943159\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 22.9 sec\n",
      " iteration: 2\n",
      "2.74229072924 1.00667372757 0.143936882999 0.0452484927322 0.930463983753 0.96261908806 0.0463748223198 0.95362517768\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.0 sec\n",
      " iteration: 3\n",
      "2.72914509715 0.832884327236 0.140324535045 0.0501714466798 0.933065852702 0.970884956893 0.0510267935435 0.948973206456\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 22.8 sec\n",
      " iteration: 4\n",
      "3.23533544258 0.769296864527 0.143174363356 0.0493198138423 0.964825779987 0.941562536919 0.0486318869227 0.951368113077\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 22.2 sec\n",
      " iteration: 5\n",
      "4.02750923094 0.713697149589 0.144242085508 0.0570979820747 0.988907186448 0.942015826864 0.0545867112257 0.945413288774\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 22.3 sec\n",
      " iteration: 6\n",
      "4.15793870146 0.631856112322 0.143351192342 0.0497807155322 0.989880944634 0.947746905739 0.0478816498092 0.952118350191\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.2 sec\n",
      " iteration: 7\n",
      "4.28427981818 0.655953583039 0.142917912089 0.0515462927728 1.05019614994 0.950010300752 0.0467861550706 0.953213844929\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.9 sec\n",
      " iteration: 8\n",
      "3.67121062886 0.587122975176 0.14195171862 0.0445896586065 0.92489000852 0.949868028318 0.0459933922479 0.954006607752\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 22.9 sec\n",
      " iteration: 9\n",
      "3.77292131302 0.54944641797 0.14169531454 0.0467619100615 0.99325978945 0.953482187382 0.0449624369217 0.955037563078\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.5 sec\n",
      " iteration: 10\n",
      "3.38891113009 0.530134452301 0.141887113764 0.0442667432841 0.924127896093 0.963117569854 0.0457114708034 0.954288529197\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.7 sec\n",
      " iteration: 11\n",
      "3.63979373331 0.537252660153 0.14124253481 0.0483903633538 1.05924501626 0.961281340532 0.043687989969 0.956312010031\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.1 sec\n",
      " iteration: 12\n",
      "4.00669333968 0.539280287845 0.141562603277 0.0422134609447 1.04488345451 0.953875257775 0.0388313685235 0.961168631476\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.0 sec\n",
      " iteration: 13\n",
      "4.21370713517 0.541470498105 0.141291670922 0.0350528310629 0.958156449816 0.952017800064 0.0352924924663 0.964707507534\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.3 sec\n",
      " iteration: 14\n",
      "4.18812845265 0.53583983826 0.14027866111 0.0335461179534 1.02355087096 0.94445092275 0.0317341911909 0.968265808809\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.9 sec\n",
      " iteration: 15\n",
      "4.23624447396 0.534947751772 0.140265046011 0.0253007539472 0.936382464426 0.94270458303 0.0263088233878 0.973691176612\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.0 sec\n",
      " iteration: 16\n",
      "4.07237706729 0.531808819884 0.140049583001 0.0246406887759 0.937455283411 0.941991090702 0.0256114665151 0.974388533485\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.0 sec\n",
      " iteration: 17\n",
      "4.13329140647 0.531980806893 0.1399823413 0.0250319634135 0.984143549137 0.938448263671 0.0248043705997 0.9751956294\n",
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.2 sec\n",
      " iteration: 18\n",
      "4.01837335465 0.533230294691 0.140344310441 0.0236737140869 0.981845572983 0.933370106315 0.0235437692656 0.976456230734\n",
      " [                  2%                  ] 22 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 0.00110354321077 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.5 sec\n",
      " iteration: 19\n",
      "4.21615509451 0.533655152026 0.140147852115 0.0250259260145 0.986272290677 0.933628288248 0.0247463365419 0.975253663458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 0.000107286497951 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.5 sec\n",
      " iteration: 20\n",
      "4.18795036412 0.531168811485 0.139839274021 0.0252677477712 0.973142450663 0.933654553154 0.0253079824414 0.974692017559\n",
      " [                  2%                  ] 23 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of -0.00125805544667 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of -0.829580012709 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.3 sec\n",
      " iteration: 21\n",
      "4.16081694085 0.531515100927 0.1396734662 0.0249518160457 0.969009277183 0.933672381067 0.0251034132178 0.974896586782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of -0.000408180756494 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 34.2651518583 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.2 sec\n",
      " iteration: 22\n",
      "4.14866678379 0.528516517392 0.139754593739 0.025350274959 0.983125192555 0.933723551536 0.0251372252232 0.974862774777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of -106.10217008 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 2.33230386554e+13 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.4 sec\n",
      " iteration: 23\n",
      "4.12584046177 0.528830615215 0.139753064019 0.0233811829616 0.978344180756 0.933723389153 0.0233409113999 0.9766590886\n",
      " [                  2%                  ] 22 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 4.14744911691e+13 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7183648776.95 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.0 sec\n",
      " iteration: 24\n",
      "4.15490252499 0.530701581116 0.139753041508 0.024219787449 0.978026758053 0.933723696955 0.0241654985569 0.975834501443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8249032553.14 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7184007959.39 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.7 sec\n",
      " iteration: 25\n",
      "4.10652858599 0.527696328771 0.13975303284 0.0242422746629 0.976744901973 0.933723841673 0.024218366857 0.975781633143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8249445004.77 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7184367159.78 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.8 sec\n",
      " iteration: 26\n",
      "4.1425204002 0.527343029024 0.139753033365 0.0236834416434 0.974976621658 0.933723734319 0.023715218535 0.976284781465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8249857477.02 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7184726378.14 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.6 sec\n",
      " iteration: 27\n",
      "4.12006996468 0.527433290085 0.139753008879 0.0227070458141 0.976300340666 0.93372371067 0.0227296075299 0.97727039247\n",
      " [                  2%                  ] 20 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8250269969.89 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7185085614.47 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.7 sec\n",
      " iteration: 28\n",
      "4.08778096303 0.526634773058 0.13975302849 0.0215447274935 0.978343480358 0.933723797477 0.0215471362941 0.978452863706\n",
      " [                  2%                  ] 23 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8250682483.39 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7185444868.75 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.9 sec\n",
      " iteration: 29\n",
      "4.03024845422 0.526190849014 0.139753036009 0.0212977422011 0.976927305264 0.933723694918 0.0213356118995 0.978664388101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8251095017.52 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7185804140.98 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 0.00010010326514 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.5 sec\n",
      " iteration: 30\n",
      "4.07460330182 0.526348297816 0.139753046173 0.0219125847446 0.981902530196 0.933723789239 0.0218293034429 0.978170696557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8251507572.27 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7186163431.2 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 49171.8258114 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.4 sec\n",
      " iteration: 31\n",
      "4.16652131527 0.526341184325 0.139753080864 0.0218615908912 0.977012114067 0.933723585103 0.0218862412562 0.978113758744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8251920147.64 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7186522739.38 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 1.23394438042e+12 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.4 sec\n",
      " iteration: 32\n",
      "4.12640792022 0.526341190856 0.139753063002 0.0222053007844 0.979061113432 0.933723433988 0.0221772152438 0.977822784756\n",
      " [                  2%                  ] 23 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8252332743.64 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7186882065.5 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 17312879281.0 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.8 sec\n",
      " iteration: 33\n",
      "4.13875178131 0.52634122843 0.139753062788 0.0224712627451 0.97947498508 0.93372347928 0.022427613052 0.977572386948\n",
      " [                  2%                  ] 21 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8252745360.28 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7187241409.61 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 17313744925.0 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.9 sec\n",
      " iteration: 34\n",
      "4.10782590833 0.526341282905 0.139753051697 0.022986464195 0.975576599528 0.933723627813 0.0230195418098 0.97698045819\n",
      " [                  2%                  ] 21 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8253157997.55 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7187600771.67 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 17314610612.2 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.1 sec\n",
      " iteration: 35\n",
      "4.07698117662 0.52634131603 0.139753065651 0.0229111440582 0.976137039065 0.933723675947 0.022932972048 0.977067027952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8253570655.45 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7187960151.72 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 17315476342.8 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1001 of 1000 complete in 23.5 sec\n",
      " iteration: 36\n",
      "4.05798549205 0.526341323637 0.139753083456 0.023770103676 0.978463766133 0.933723829446 0.0237171227116 0.976282877288\n",
      " [                  2%                  ] 22 of 1000 complete in 0.5 sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8253983333.98 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7188319549.72 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 17316342116.6 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 24.3 sec\n",
      " iteration: 37\n",
      "4.02034384653 0.526341317441 0.139753093572 0.0238976164494 0.980010023456 0.933723768961 0.0238045966575 0.976195403342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 0.000862357206643 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8254396033.16 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7188678965.7 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 17317207933.7 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 1000 of 1000 complete in 23.8 sec\n",
      " iteration: 38\n",
      "4.02752750799 0.5263413635 0.139753093548 0.0234640050176 0.975212884497 0.933723766757 0.0234950916197 0.97650490838\n",
      "Converged\n",
      "{'b_h': 0.976504908380317, 'a_s': 4.0275275079856891, 'r_0': 0.13975309354792539, 'kappa': 0.93372376675686086, 'b_s': 0.52634136350014238, 'a_h': 0.023495091619682999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 2.13303700582e+12 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 8254808752.95 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 7189038399.66 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/harit.vishwakarma/.h/miniconda2/lib/python2.7/site-packages/scipy/optimize/zeros.py:173: RuntimeWarning: Tolerance of 17318073794.1 reached\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#htrm.fit_mle_params(tol=1e-18)\n",
    "htrm.add_realization(\"train\",T[:30])\n",
    "htrm.set_cur_realization_key(\"train\")\n",
    "htrm.fit_em_map_params()\n",
    "print htrm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#htrm.plot()\n",
    "print htrm.log_likelihood()\n",
    "print htrm.log_likelihood_1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HTRm Sampling Demo\n",
    "# Reproducing figures 3 A-E of the paper.\n",
    "# Figure 3.A\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "htrm = HTRm(a_s=0.0,b_s=1.0,r_0=0.1,kappa=1.0,a_h=0.0,b_h=1.0,T_s=[])\n",
    "\n",
    "T = htrm.sample(n_samples=100,t_0=1.0)\n",
    "htrm.plot()\n",
    "print htrm.log_likelihood_1()\n",
    "print htrm.coef_variation()\n",
    "print htrm.iei_corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure 3.B\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "#params = {\"a_s\":0.0, \"b_s\":1.0, \"r_0\":0.1, \"t_0\":1.0, \"kappa\":1.0, \"a_h\":1.5, \"b_h\":2.0,\"T_s\":T_s}\n",
    "htrm = HTRm(a_s=0.0,b_s=1.0,r_0=0.1,kappa=3.0,a_h=0.0,b_h=1.0,T_s=[])\n",
    "\n",
    "T = htrm.sample(n_samples=100,t_0=1.0)\n",
    "htrm.plot()\n",
    "print htrm.log_likelihood()\n",
    "print htrm.coef_variation()\n",
    "print htrm.iei_corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure 3.C\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "htrm = HTRm(a_s=0.0,b_s=1.0,r_0=0.1,kappa=0.7,a_h=0.0,b_h=1.0,T_s=[])\n",
    "\n",
    "T = htrm.sample(n_samples=100,t_0=1.0)\n",
    "\n",
    "htrm.plot()\n",
    "print htrm.coef_variation()\n",
    "print htrm.iei_corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Figure 3D\n",
    "np.random.seed(0)\n",
    "htrm = HTRm(a_s=0.0,b_s=1.0,r_0=0.1,kappa=1.0,a_h=1.5,b_h=2.0,T_s=[])\n",
    "T = htrm.sample(n_samples=100,t_0=1.0)\n",
    "\n",
    "htrm.plot()\n",
    "\n",
    "print htrm.coef_variation()\n",
    "print htrm.iei_corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fig 3E\n",
    "np.random.seed(0)\n",
    "htrm = HTRm(a_s=0.0,b_s=1.0,r_0=0.1,kappa=1.5,a_h=1.8,b_h=4.0,T_s=[])\n",
    "T = htrm.sample(n_samples=100,t_0=1.0)\n",
    "htrm.plot()\n",
    "print htrm.log_likelihood_1()\n",
    "print htrm.coef_variation()\n",
    "print htrm.iei_corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fig 3F\n",
    "htrm = HTRm(a_s=2.0,b_s=0.5,r_0=0.1,kappa=1.0,a_h=0.0,b_h=4.0,T_s=[20,160])\n",
    "T = htrm.sample(n_samples=50,t_0=1.0)\n",
    "\n",
    "htrm.plot()\n",
    "print htrm.log_likelihood_1()\n",
    "print htrm.coef_variation()\n",
    "print htrm.iei_corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class X:\n",
    "    def __init__(self,x=5):\n",
    "        self.x = x\n",
    "    def a(self):\n",
    "        z = 20\n",
    "        def b(y=6):\n",
    "            print self.x+y\n",
    "            print z\n",
    "            print d\n",
    "            #z = 5\n",
    "        d = 5\n",
    "        b()\n",
    "        d = 30\n",
    "        b()\n",
    "        print z\n",
    "xx = X()\n",
    "xx.x = 6\n",
    "xx.a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
